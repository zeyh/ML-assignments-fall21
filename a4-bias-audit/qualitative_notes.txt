I run the training dataset with cross-analysis and the filter of either "entailment", "neutual", or "contradiction". Given the gender intensive pronouns such as "women, woman, she, her" and "men, man, he, his" and calculating the top 10 words that has the highest PMI, here are some of my observations:
In describing a single man or woman, to give an entailment or contradiction, people turns to think of concrete appearance related words such as "headscarf" and "handbags" for woman, or "tuxedo" or "mustache" for man. For the action words, "she" is more assiciated with "tears" and "peeling", while "he" is more likely to be associated with "flour", "finishes", or "guide".  However, for the neutual task that requires the anotator writing something that might be true, it seems that people have more imaginations in this category [1]. "Woman" associates with words like "seductress" and "prostitute" that might indicate an action to another person, while "man" associates with words like "balding", "plumber", or "shoeshine", which is more about describing their own properties. Given those results, I'm hesitate to make any solid conclusions without some detailed demographic descriptions of people who generated those sentences. Also, it is unclear if the initial provided images have hidden bias inside, since when I run the top 50 associations with a neutual pronoun "they", the highest association amount all filters are "ponies" which I'm not sure why. However, the gender bias is clearly exists between those associations. 
Given the observation, it is a rising concern that many NLP models are often pretrained given a large corpus, such that there are higher probability to introduce those existing bais to generative models. Moreover, Sharma *et al.* further discussed this issue by fine-tuning a selection of pretrained models to detect whether data, or the model itself, plays more rule in introducing the bias for language inference tasks. They also introduced data augmentation techniques for debiasing [2]. 

[1] Bowman, Samuel R., et al. "A large annotated corpus for learning natural language inference." *arXiv preprint arXiv:1508.05326* (2015).
[2] Sharma, Shanya, Manan Dey, and Koustuv Sinha. "Evaluating Gender Bias in Natural Language Inference." *arXiv preprint arXiv:2105.05541* (2021).